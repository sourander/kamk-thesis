{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323cff1c",
   "metadata": {},
   "source": [
    "## DataFrame RDD Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7dd5254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import spark_partition_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2776d814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "spark = SparkSession.builder.appName(\"RDD_exploration\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "bfac0a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.app.name               : RDD_exploration\n",
      "spark.sql.shuffle.partitions : 200\n",
      "spark.master                 : local[*]\n"
     ]
    }
   ],
   "source": [
    "# Configurations whose values we want to see\n",
    "configs_to_exam = [\"spark.app.name\", \"spark.sql.shuffle.partitions\", \"spark.master\"]\n",
    "\n",
    "# Get the lenghts of the longest key\n",
    "w = len(max(configs_to_exam, key=len))\n",
    "\n",
    "# Print\n",
    "for key in configs_to_exam:\n",
    "    val = spark.conf.get(key)\n",
    "    print(f\"{key:<{w}} : {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "38b6f2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of CPUs of Threads available:  12\n"
     ]
    }
   ],
   "source": [
    "print(\"# of CPUs of Threads available: \", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6fb92",
   "metadata": {},
   "source": [
    "## Load a DataFrame\n",
    "\n",
    "The source directory has only 1 file since we have repartitioned it before writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5e03b515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of files in source path: 1\n",
      "# of partitions in DataFrame: 1\n",
      "+-------------------+---+-----------+--------+-------------+-------------------+-------------------+---+\n",
      "|      dms_timestamp| id|customer_fk|model_fk|serial_number|            created|           modified|pid|\n",
      "+-------------------+---+-----------+--------+-------------+-------------------+-------------------+---+\n",
      "|2021-08-05 15:19:54|  1|          1|       1|  862-86-8047|1970-01-15 12:18:54|1970-01-15 12:52:11|  0|\n",
      "|2021-08-05 15:19:54|  2|          1|       1|  329-08-2350|1970-01-15 12:18:54|1970-01-15 12:50:41|  0|\n",
      "|2021-08-05 15:19:54|  3|          1|       3|  360-73-1379|2020-01-15 16:04:02|2020-01-15 16:47:19|  0|\n",
      "|2021-08-05 15:19:54|  4|          1|       3|  034-94-0243|2020-01-15 16:04:02|2020-01-15 16:57:24|  0|\n",
      "|2021-08-05 15:19:54|  5|          2|       1|  688-21-1124|2020-01-15 16:10:11|2020-01-15 16:45:38|  0|\n",
      "|2021-08-05 15:19:54|  6|          2|       2|  531-52-1018|2020-01-15 16:10:11|2020-01-15 16:59:34|  0|\n",
      "|2021-08-05 15:19:54|  7|          3|       4|  225-91-3334|2020-01-15 16:14:20|2020-01-15 16:59:47|  0|\n",
      "|2021-08-05 15:19:54|  8|          4|       2|  339-08-2633|2020-01-15 18:06:51|2020-01-15 18:06:53|  0|\n",
      "|2021-08-05 15:19:54|  9|          5|       3|  839-26-4038|2020-01-15 18:04:49|2020-01-15 18:36:36|  0|\n",
      "|2021-08-05 15:19:54| 10|          5|       1|  624-20-4847|2020-01-15 18:04:49|2020-01-15 18:11:13|  0|\n",
      "|2021-08-05 15:19:54| 11|          6|       3|  675-02-5099|2020-01-15 18:35:02|2020-01-15 19:12:29|  0|\n",
      "|2021-08-05 15:19:54| 12|          6|       3|  378-27-9930|2020-01-15 18:35:02|2020-01-15 19:14:25|  0|\n",
      "|2021-08-05 15:19:54| 13|          7|       3|  701-02-5464|2020-01-15 19:30:47|2020-01-15 20:01:17|  0|\n",
      "|2021-08-05 15:19:54| 14|          8|       4|  764-48-5189|2020-01-15 19:56:45|2020-01-15 20:20:26|  0|\n",
      "|2021-08-05 15:19:54| 15|          8|       4|  392-73-4056|2020-01-15 19:56:45|2020-01-15 20:14:29|  0|\n",
      "|2021-08-05 15:19:54| 16|          9|       1|  380-76-3156|2020-01-15 20:42:34|2020-01-15 21:07:57|  0|\n",
      "+-------------------+---+-----------+--------+-------------+-------------------+-------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = os.path.join(\"..\", \"S3\", \"staging\", \"dms\", \"abc\", \"devices\", \"devices\")\n",
    "\n",
    "df = spark.read.format(\"parquet\").load(p)\n",
    "\n",
    "print(\"# of files in source path:\", len([f for f in os.listdir(p) if f.endswith(\".parquet\")]))\n",
    "print(\"# of partitions in DataFrame:\", df.rdd.getNumPartitions())\n",
    "\n",
    "df.withColumn(\"pid\", spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a72bcabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of partitions in DataFrame: 16\n",
      "+-------------------+---+-----------+--------+-------------+-------------------+-------------------+---+\n",
      "|      dms_timestamp| id|customer_fk|model_fk|serial_number|            created|           modified|pid|\n",
      "+-------------------+---+-----------+--------+-------------+-------------------+-------------------+---+\n",
      "|2021-08-05 15:19:54|  2|          1|       1|  329-08-2350|1970-01-15 12:18:54|1970-01-15 12:50:41|  0|\n",
      "|2021-08-05 15:19:54|  1|          1|       1|  862-86-8047|1970-01-15 12:18:54|1970-01-15 12:52:11|  1|\n",
      "|2021-08-05 15:19:54|  5|          2|       1|  688-21-1124|2020-01-15 16:10:11|2020-01-15 16:45:38|  2|\n",
      "|2021-08-05 15:19:54|  3|          1|       3|  360-73-1379|2020-01-15 16:04:02|2020-01-15 16:47:19|  3|\n",
      "|2021-08-05 15:19:54|  4|          1|       3|  034-94-0243|2020-01-15 16:04:02|2020-01-15 16:57:24|  4|\n",
      "|2021-08-05 15:19:54|  6|          2|       2|  531-52-1018|2020-01-15 16:10:11|2020-01-15 16:59:34|  5|\n",
      "|2021-08-05 15:19:54|  7|          3|       4|  225-91-3334|2020-01-15 16:14:20|2020-01-15 16:59:47|  6|\n",
      "|2021-08-05 15:19:54|  8|          4|       2|  339-08-2633|2020-01-15 18:06:51|2020-01-15 18:06:53|  7|\n",
      "|2021-08-05 15:19:54| 10|          5|       1|  624-20-4847|2020-01-15 18:04:49|2020-01-15 18:11:13|  8|\n",
      "|2021-08-05 15:19:54|  9|          5|       3|  839-26-4038|2020-01-15 18:04:49|2020-01-15 18:36:36|  9|\n",
      "|2021-08-05 15:19:54| 11|          6|       3|  675-02-5099|2020-01-15 18:35:02|2020-01-15 19:12:29| 10|\n",
      "|2021-08-05 15:19:54| 12|          6|       3|  378-27-9930|2020-01-15 18:35:02|2020-01-15 19:14:25| 11|\n",
      "|2021-08-05 15:19:54| 13|          7|       3|  701-02-5464|2020-01-15 19:30:47|2020-01-15 20:01:17| 12|\n",
      "|2021-08-05 15:19:54| 15|          8|       4|  392-73-4056|2020-01-15 19:56:45|2020-01-15 20:14:29| 13|\n",
      "|2021-08-05 15:19:54| 14|          8|       4|  764-48-5189|2020-01-15 19:56:45|2020-01-15 20:20:26| 14|\n",
      "|2021-08-05 15:19:54| 16|          9|       1|  380-76-3156|2020-01-15 20:42:34|2020-01-15 21:07:57| 15|\n",
      "+-------------------+---+-----------+--------+-------------+-------------------+-------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sorted = df.sort(\"modified\")\n",
    "\n",
    "print(\"# of partitions in DataFrame:\", df_sorted.rdd.getNumPartitions())\n",
    "\n",
    "df_sorted.withColumn(\"pid\", spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "dcb23702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-----------+--------+-------------+-------------------+-------------------+---+\n",
      "|      dms_timestamp| id|customer_fk|model_fk|serial_number|            created|           modified|pid|\n",
      "+-------------------+---+-----------+--------+-------------+-------------------+-------------------+---+\n",
      "|2021-08-05 15:19:54|  2|          1|       1|  329-08-2350|1970-01-15 12:18:54|1970-01-15 12:50:41|  0|\n",
      "|2021-08-05 15:19:54|  1|          1|       1|  862-86-8047|1970-01-15 12:18:54|1970-01-15 12:52:11|  0|\n",
      "|2021-08-05 15:19:54|  5|          2|       1|  688-21-1124|2020-01-15 16:10:11|2020-01-15 16:45:38|  0|\n",
      "|2021-08-05 15:19:54|  3|          1|       3|  360-73-1379|2020-01-15 16:04:02|2020-01-15 16:47:19|  0|\n",
      "|2021-08-05 15:19:54|  4|          1|       3|  034-94-0243|2020-01-15 16:04:02|2020-01-15 16:57:24|  1|\n",
      "|2021-08-05 15:19:54|  6|          2|       2|  531-52-1018|2020-01-15 16:10:11|2020-01-15 16:59:34|  1|\n",
      "|2021-08-05 15:19:54|  7|          3|       4|  225-91-3334|2020-01-15 16:14:20|2020-01-15 16:59:47|  1|\n",
      "|2021-08-05 15:19:54|  8|          4|       2|  339-08-2633|2020-01-15 18:06:51|2020-01-15 18:06:53|  1|\n",
      "|2021-08-05 15:19:54| 10|          5|       1|  624-20-4847|2020-01-15 18:04:49|2020-01-15 18:11:13|  2|\n",
      "|2021-08-05 15:19:54|  9|          5|       3|  839-26-4038|2020-01-15 18:04:49|2020-01-15 18:36:36|  2|\n",
      "|2021-08-05 15:19:54| 11|          6|       3|  675-02-5099|2020-01-15 18:35:02|2020-01-15 19:12:29|  2|\n",
      "|2021-08-05 15:19:54| 12|          6|       3|  378-27-9930|2020-01-15 18:35:02|2020-01-15 19:14:25|  2|\n",
      "|2021-08-05 15:19:54| 13|          7|       3|  701-02-5464|2020-01-15 19:30:47|2020-01-15 20:01:17|  3|\n",
      "|2021-08-05 15:19:54| 15|          8|       4|  392-73-4056|2020-01-15 19:56:45|2020-01-15 20:14:29|  3|\n",
      "|2021-08-05 15:19:54| 14|          8|       4|  764-48-5189|2020-01-15 19:56:45|2020-01-15 20:20:26|  3|\n",
      "|2021-08-05 15:19:54| 16|          9|       1|  380-76-3156|2020-01-15 20:42:34|2020-01-15 21:07:57|  3|\n",
      "+-------------------+---+-----------+--------+-------------+-------------------+-------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sorted.coalesce(4).withColumn(\"pid\", spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275fcd5c",
   "metadata": {},
   "source": [
    "## Generate a DataFrame\n",
    "\n",
    "Instead of loading a DataFrame from a single Parquet file, let's see what happens when we generate one from made up data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e0dcdecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of partitions in DataFrame: 12\n",
      "+---+-----------+---+\n",
      "| id|     sample|pid|\n",
      "+---+-----------+---+\n",
      "|  0|  0.3667918|  0|\n",
      "|  1|  0.8227412|  0|\n",
      "|  2|  0.8557527|  0|\n",
      "|  3| 0.09284008|  0|\n",
      "|  4| 0.43896046|  0|\n",
      "|  5|0.029582817|  0|\n",
      "|  6| 0.75651675|  0|\n",
      "|  7|0.037324116|  0|\n",
      "|  8| 0.07729287|  0|\n",
      "|  9|  0.6893575|  0|\n",
      "| 10|   0.695219|  0|\n",
      "| 11|  0.4964869|  0|\n",
      "| 12| 0.29565847|  0|\n",
      "| 13|  0.7083751|  0|\n",
      "| 14| 0.34966978|  0|\n",
      "| 15|  0.8800423|  0|\n",
      "| 16| 0.45173523|  0|\n",
      "| 17|  0.8067894|  0|\n",
      "| 18|  0.6379171|  0|\n",
      "| 19| 0.17989096|  0|\n",
      "+---+-----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define some random data\n",
    "data = [{\"id\": i, \"sample\": random.random()} for i in range(1000)]\n",
    "\n",
    "# Define schema with DDL syntax\n",
    "schema = \"id INT, sample FLOAT\"\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "print(\"# of partitions in DataFrame:\", df.rdd.getNumPartitions())\n",
    "df.withColumn(\"pid\", spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9674bb4e",
   "metadata": {},
   "source": [
    "### Notice that...\n",
    "\n",
    "We get as many RDD partitions as there are CPU Threads.\n",
    "\n",
    "Let's shuffle the data and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2572ab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of partitions in DataFrame: 200\n",
      "+---+------------+---+\n",
      "| id|      sample|pid|\n",
      "+---+------------+---+\n",
      "|163| 5.330911E-4|  0|\n",
      "|119| 8.778424E-4|  0|\n",
      "|750|0.0057204547|  0|\n",
      "|275| 0.008876068|  0|\n",
      "|903| 0.009868193|  0|\n",
      "|479| 0.009877583|  1|\n",
      "|318| 0.013418788|  1|\n",
      "|627| 0.014248454|  1|\n",
      "|187| 0.014699633|  1|\n",
      "|489| 0.015786262|  1|\n",
      "| 20| 0.017281093|  2|\n",
      "|342| 0.019262396|  2|\n",
      "|564| 0.019438043|  2|\n",
      "|717| 0.020404326|  2|\n",
      "|567| 0.021412997|  2|\n",
      "|963| 0.023174021|  3|\n",
      "|234|  0.02571832|  3|\n",
      "|853|  0.02658498|  3|\n",
      "|406|  0.02842313|  3|\n",
      "|209| 0.029365541|  3|\n",
      "+---+------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sorted = df.sort(\"sample\")\n",
    "\n",
    "print(\"# of partitions in DataFrame:\", df_sorted.rdd.getNumPartitions())\n",
    "df_sorted.withColumn(\"pid\", spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02159cd7",
   "metadata": {},
   "source": [
    "### Notice that...\n",
    "\n",
    "We get as many RDD partitions after shuffling as in `spark.sql.shuffle.partitions`. The previous example, where we loaded a file, was limited by the number of rows.\n",
    "\n",
    "Let's change this settings and perform the sort again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "30d3d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "799ebbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of partitions in DataFrame: 24\n",
      "+---+------------+---+\n",
      "| id|      sample|pid|\n",
      "+---+------------+---+\n",
      "|163| 5.330911E-4|  0|\n",
      "|119| 8.778424E-4|  0|\n",
      "|750|0.0057204547|  0|\n",
      "|275| 0.008876068|  0|\n",
      "|903| 0.009868193|  0|\n",
      "|479| 0.009877583|  0|\n",
      "|318| 0.013418788|  0|\n",
      "|627| 0.014248454|  0|\n",
      "|187| 0.014699633|  0|\n",
      "|489| 0.015786262|  0|\n",
      "| 20| 0.017281093|  0|\n",
      "|342| 0.019262396|  0|\n",
      "|564| 0.019438043|  0|\n",
      "|717| 0.020404326|  0|\n",
      "|567| 0.021412997|  0|\n",
      "|963| 0.023174021|  0|\n",
      "|234|  0.02571832|  0|\n",
      "|853|  0.02658498|  0|\n",
      "|406|  0.02842313|  0|\n",
      "|209| 0.029365541|  0|\n",
      "+---+------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sorted = df.sort(\"sample\").withColumn(\"pid\", spark_partition_id())\n",
    "\n",
    "print(\"# of partitions in DataFrame:\", df_sorted.rdd.getNumPartitions())\n",
    "df_sorted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32d737",
   "metadata": {},
   "source": [
    "### How are they distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d5403165",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.createOrReplaceTempView(\"sorted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e6d0fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "counted = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  pid, \n",
    "  count(*) as n_elems \n",
    "FROM sorted \n",
    "GROUP BY pid\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4bdeab5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 42),\n",
       " (1, 42),\n",
       " (2, 41),\n",
       " (3, 42),\n",
       " (4, 42),\n",
       " (5, 41),\n",
       " (6, 42),\n",
       " (7, 42),\n",
       " (8, 41),\n",
       " (9, 42),\n",
       " (10, 42),\n",
       " (11, 42),\n",
       " (12, 41),\n",
       " (13, 42),\n",
       " (14, 41),\n",
       " (15, 42),\n",
       " (16, 42),\n",
       " (17, 41),\n",
       " (18, 42),\n",
       " (19, 42),\n",
       " (20, 41),\n",
       " (21, 42),\n",
       " (22, 42),\n",
       " (23, 41)]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(x.pid, x.n_elems) for x in counted])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
