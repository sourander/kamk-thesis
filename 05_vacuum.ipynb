{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2df0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from helpers.paths import PathMerger\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b594c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"VacuumApp\")\n",
    "         .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")\n",
    "         .config('spark.sql.extensions', \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "         .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "         .getOrCreate())\n",
    "\n",
    "\n",
    "# This cannot be imported before initializing the SparkSession.\n",
    "from delta import DeltaTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf7185d",
   "metadata": {},
   "source": [
    "## Load Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e0aec84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Params\n",
    "db, table = \"devices\", \"device_models\"\n",
    "all_pks = [\"id\"]\n",
    "\n",
    "# Init\n",
    "pm = PathMerger(db, table)\n",
    "\n",
    "# Create BRONZE\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze\")\n",
    "\n",
    "# Even on Windows, Spark SQL requires a POSIX path with /-symbol as path separator.\n",
    "abs_path = os.path.abspath(pm.bronze).replace(\"\\\\\", \"/\")\n",
    "\n",
    "# Register to Hive\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {pm.hive}\n",
    "  USING DELTA\n",
    "  LOCATION '{abs_path}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb1e59be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+------------+----------------+-----+-----------+-------------------+-------------------+------------+\n",
      "|      dms_timestamp| id|release_date|            name|color|description|            created|           modified|src_batch_id|\n",
      "+-------------------+---+------------+----------------+-----+-----------+-------------------+-------------------+------------+\n",
      "|2021-09-11 11:39:29|  2|  2010-05-15|Super Gadget 100|Black|update id 2|2010-03-21 12:00:02|2021-09-11 11:39:29|           0|\n",
      "|2021-09-11 11:37:17|  6|  2021-12-31|Super Gadget 300| Pink| new device|2021-09-11 11:37:17|2021-09-11 11:37:17|           0|\n",
      "|2021-09-11 11:37:47|  1|  2010-05-15|Super Gadget 100|  Red|update id 1|2010-03-21 12:00:01|2021-09-11 11:37:47|           0|\n",
      "|2021-09-11 11:30:04|  4|  2018-05-13|Super Gadget 200|White|lorem ipsum|2018-03-20 12:01:01|2018-03-20 12:01:01|        null|\n",
      "|2021-09-11 11:30:04|  3|  2010-11-01|Super Gadget 100| Pink|lorem ipsum|2010-08-05 07:00:00|2010-08-05 07:00:00|        null|\n",
      "+-------------------+---+------------+----------------+-----+-----------+-------------------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"SELECT * FROM {pm.hive}\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e6d550",
   "metadata": {},
   "source": [
    "# Vacuum Configuration\n",
    "\n",
    "By default, the VACUUM operation will delete files older than 168 hours. Setting the retain duration smaller than this will raise an exception unless we disable the `retentionDurationCheck` in configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf88000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad5256",
   "metadata": {},
   "source": [
    "# Check what files we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79aa673",
   "metadata": {},
   "source": [
    "### According to the table itself\n",
    "\n",
    "Notice that there is one roque file. It is not part of the `SELECT *` query. If you were to read the file directly, you would notice that is is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a31801ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Sparks function for getting filepaths of all rows.\n",
    "files_in_use = spark.sql(f\"SELECT DISTINCT input_file_name() as path FROM {pm.hive}\").collect()\n",
    "\n",
    "# Find the current working dir name\n",
    "cwd = os.path.basename(os.getcwd())\n",
    "\n",
    "files_currently_used = []\n",
    "\n",
    "for fiu in files_in_use:\n",
    "    # Split into subdirs\n",
    "    fiu_parts = fiu.path.split(\"/\")\n",
    "    \n",
    "    # Find the index of current working dir\n",
    "    idx = fiu_parts.index(cwd)\n",
    "    \n",
    "    # Start at the first dir inside the cwd\n",
    "    parquet_path = os.path.sep.join(fiu_parts[idx+1:])\n",
    "    \n",
    "    files_currently_used.append(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "002820fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Mark files that are NOT part of the query: \n",
      "[x] S3\\bronze\\abc\\devices\\device_models\\part-00000-5a0ae9da-f510-4400-8377-9a1ed88cb259-c000.parquet\n",
      "[x] S3\\bronze\\abc\\devices\\device_models\\part-00000-d5cf549d-bdb9-4d07-9eb6-a84aeb5a4aaa-c000.snappy.parquet\n",
      "[ ] S3\\bronze\\abc\\devices\\device_models\\part-00045-2193ab89-242b-413d-811c-9bd3f6d19bdc-c000.snappy.parquet\n",
      "[ ] S3\\bronze\\abc\\devices\\device_models\\part-00069-78a281b9-e5c6-4d90-abf1-3a9fe31afb59-c000.snappy.parquet\n",
      "[ ] S3\\bronze\\abc\\devices\\device_models\\part-00107-513e811f-e296-4c9b-89f8-c204a3198269-c000.snappy.parquet\n",
      "[ ] S3\\bronze\\abc\\devices\\device_models\\part-00128-2d0c9196-a633-4efb-9645-758b76739c26-c000.snappy.parquet\n",
      "[ ] S3\\bronze\\abc\\devices\\device_models\\part-00140-8a607eac-4b5f-4b41-bab3-98e085012dac-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Mark files that are NOT part of the query: \")\n",
    "\n",
    "for f in glob.glob(pm.bronze + os.sep + \"**/*.parquet\", recursive=True):\n",
    "    check = 'x' if f not in files_currently_used else ' '\n",
    "    print(f\"[{check}] {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b446702",
   "metadata": {},
   "source": [
    "### According to the latest commit in delta log\n",
    "\n",
    "Notice that the log lists also the empty file as a part of this delta table version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afebe748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Opening: 00000000000000000001.json\n"
     ]
    }
   ],
   "source": [
    "# Find the Latest \n",
    "all_json_paths = [f for f in glob.glob(os.path.join(pm.bronze, '_delta_log') + os.sep + '*.json')]\n",
    "latest_json_path = max(all_json_paths)\n",
    "\n",
    "# Memoization\n",
    "files_added_in_log = []\n",
    "files_removed_in_log = [] \n",
    "\n",
    "print(f\"[INFO] Opening: {os.path.basename(latest_json_path)}\")\n",
    "\n",
    "with open(latest_json_path, 'r') as f:\n",
    "    json_rows = f.readlines()\n",
    "    \n",
    "    for j in json_rows:\n",
    "        json_data = json.loads(j)\n",
    "        \n",
    "        if 'add' in json_data.keys():\n",
    "            parquet_path = os.path.join(pm.bronze, json_data['add']['path'])\n",
    "            files_added_in_log.append(parquet_path)\n",
    "            \n",
    "        if 'remove' in json_data.keys():\n",
    "            parquet_path = os.path.join(pm.bronze, json_data['remove']['path'])\n",
    "            files_removed_in_log.append(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f1b4014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Mark files that were removed from delta table during commit: \n",
      "[x] part-00000-5a0ae9da-f510-4400-8377-9a1ed88cb259-c000.parquet          2.51 KB\n",
      "[ ] part-00000-d5cf549d-bdb9-4d07-9eb6-a84aeb5a4aaa-c000.snappy.parquet   0.98 KB\n",
      "[ ] part-00045-2193ab89-242b-413d-811c-9bd3f6d19bdc-c000.snappy.parquet   2.66 KB\n",
      "[ ] part-00069-78a281b9-e5c6-4d90-abf1-3a9fe31afb59-c000.snappy.parquet   2.66 KB\n",
      "[ ] part-00107-513e811f-e296-4c9b-89f8-c204a3198269-c000.snappy.parquet   2.62 KB\n",
      "[ ] part-00128-2d0c9196-a633-4efb-9645-758b76739c26-c000.snappy.parquet   2.68 KB\n",
      "[ ] part-00140-8a607eac-4b5f-4b41-bab3-98e085012dac-c000.snappy.parquet   2.63 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Mark files that were removed from delta table during commit: \")\n",
    "\n",
    "for f in glob.glob(pm.bronze + os.sep + \"**/*.parquet\", recursive=True):\n",
    "    check = 'x' if f in files_removed_in_log else ' '\n",
    "    size = round(os.stat(f).st_size / 1024, 2)\n",
    "    print(f\"[{check}] {os.path.basename(f):<69} {size} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bff6b19",
   "metadata": {},
   "source": [
    "## Check what files VACUUM would delete\n",
    "\n",
    "Note that we will perform a dry run. It will not delete any files; it just shows what would be deleted if `DRY RUN` statement would be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc7e485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_run_files = [x.path for x in spark.sql(f\"VACUUM {pm.hive} RETAIN 24 HOURS DRY RUN\").collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bb7a2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The files that would be deleted by VACUUM\n",
      "S3\\bronze\\abc\\devices\\device_models\\part-00000-5a0ae9da-f510-4400-8377-9a1ed88cb259-c000.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] The files that would be deleted by VACUUM\")\n",
    "for f in dry_run_files:\n",
    "    \n",
    "    f_path = os.path.sep.join(f.split(\"/\")[idx+1:])\n",
    "    \n",
    "    print(f_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
