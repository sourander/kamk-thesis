{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e9ec06",
   "metadata": {},
   "source": [
    "# Merge the CDC Files to Bronze\n",
    "\n",
    "Note that in Databricks, we could use the Auto Loader. In this case, our **readStream** would have additional options:\n",
    "\n",
    "```python\n",
    " .format(\"cloudFiles\")\n",
    " .option(\"cloudFiles.format\", \"parquet\")\n",
    " .option(\"cloudFiles.useNotifications\", \"true\") # Use for SQS/SNS\n",
    " .option(\"cloudFiles.region\", \"eu-west-1\")      # Use for SQS/SNS\n",
    "```\n",
    "\n",
    "My thesis includes examples of this in Finnish. This script uses file listing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8621f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pprint\n",
    "import pyspark.sql.functions as F\n",
    "from helpers.paths import PathMerger\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52978122",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"MergeCDCtoBronze\")\n",
    "         .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")\n",
    "         .config('spark.sql.extensions', \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "         .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "         .getOrCreate())\n",
    "\n",
    "\n",
    "# This cannot be imported before initializing the SparkSession.\n",
    "from delta import DeltaTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c68258",
   "metadata": {},
   "source": [
    "## Imagine orchestator here\n",
    "\n",
    "If this was a worker Notebook in Databricks (or a worker Python script orchestrated by Airflow) the parameters below would be fed while executing this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbbcfcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "db, table = \"devices\", \"device_models\"\n",
    "all_pks = [\"id\"]\n",
    "\n",
    "# Init\n",
    "pm = PathMerger(db, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6f18939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The following Parquet files exist in this staging path: \n",
      "S3\\staging\\dms\\abc\\devices\\device_models\\LOAD00000001.parquet\n",
      "S3\\staging\\dms\\abc\\devices\\device_models\\2021\\9\\11\\20210911_113913.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] The following Parquet files exist in this staging path: \")\n",
    "\n",
    "for f in glob.glob(pm.staging + os.sep + \"**/*.parquet\", recursive=True):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b80e0d",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "Based on my testing, the pathGlobFilter applies to the filename, not to the whole path. \n",
    "\n",
    "Thus, a glob filter such as...\n",
    "* `**/*.parquet` returns no files\n",
    "* `[L]*.parquet` returns all files starting with an `L` letter and ending to `.parquet`.\n",
    "* `[!L]*.parquet` returns all files NOT starting with an `L` letter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b66f9b",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dda0cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_ordering_cols(input_df, batch_id):\n",
    "    output_df = ( \n",
    "        input_df\n",
    "        .withColumn(\"op_numeral\", F.when(F.col(\"Op\") == \"I\", 1)\n",
    "                                     .when(F.col(\"Op\") == \"U\", 2)\n",
    "                                     .when(F.col(\"Op\") == \"D\", 3).cast(\"int\"))\n",
    "        .withColumn('dms_temp', F.to_timestamp(F.col(\"dms_timestamp\")))\n",
    "        #.withColumn(\"par\", F.col(\"*all_pks[0]) % n_pars)\n",
    "        #.withColumn(\"src_file\", F.input_file_name())\n",
    "        .withColumn(\"src_batch_id\", F.lit(batch_id).cast(\"int\"))\n",
    "    )\n",
    "    return output_df \n",
    "\n",
    "\n",
    "def log_compact(input_df, cols_to_drop=[\"aaa\", \"bbb\"]):\n",
    "    output_df = (\n",
    "        input_df\n",
    "            .selectExpr(*all_pks, \"struct(dms_temp as aaa, op_numeral as bbb, *) as others\")\n",
    "            .groupBy(*all_pks)\n",
    "            .agg(F.max(\"others\").alias(\"latest\"))\n",
    "            .select(\"latest.*\")\n",
    "            .drop(*cols_to_drop)\n",
    "        \n",
    "    )\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def merge_to_delta(batch_df, batch_id):\n",
    "    \n",
    "    # Add op_numeral and dms_temp\n",
    "    batch_df = with_ordering_cols(batch_df, batch_id)\n",
    "    \n",
    "    # Compact change log to one item per id\n",
    "    latest_uniques = log_compact(batch_df)\n",
    "    \n",
    "    # Load Delta Table\n",
    "    target = DeltaTable.forName(spark, pm.hive)\n",
    "    \n",
    "    # Using target schema, format to: { \"id\": \"s.id\" }\n",
    "    col_map = {x.name: f\"s.{x.name}\" for x in target.toDF().schema}\n",
    "    \n",
    "    # Format the list of primary keys \n",
    "    # into SQL join condition like \"t.id = s.id AND t.foo = s.foo\"\n",
    "    join_cond = \" AND \".join([f\"t.{pk} = s.{pk}\" for pk in all_pks])\n",
    "    \n",
    "    (\n",
    "      target.alias(\"t\")\n",
    "      .merge(\n",
    "          source = latest_uniques.alias(\"s\"), \n",
    "          condition = f\"{join_cond}\"\n",
    "      )\n",
    "      .whenMatchedDelete(condition = \"s.Op = 'D'\")\n",
    "      .whenMatchedUpdate(condition = \"s.Op = 'U'\", set = col_map)\n",
    "      .whenNotMatchedInsert(condition = \"s.Op != 'D'\", values = col_map)\n",
    "      .execute()\n",
    "    )    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5620a164",
   "metadata": {},
   "source": [
    "## Register to Hive\n",
    "\n",
    "The Hive in single-node test environment is not persistent, so we will need to create a new database and a new (EXTERNAL) table each time we restart our Python kernel and create a new SparkSession.\n",
    "\n",
    "In production, this would not be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd6f3167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create BRONZE\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze\")\n",
    "\n",
    "# Even on Windows, Spark SQL requires a POSIX path with /-symbol as path separator.\n",
    "abs_path = os.path.abspath(pm.bronze).replace(\"\\\\\", \"/\")\n",
    "\n",
    "# Register to Hive\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {pm.hive}\n",
    "  USING DELTA\n",
    "  LOCATION '{abs_path}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8529a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-09-12 06:26:03.746</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CREATE OR REPLACE TABLE AS SELECT</td>\n",
       "      <td>{'description': None, 'partitionBy': '[]', 'pr...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '4', 'numOutputBytes': '2569...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName  \\\n",
       "0        0 2021-09-12 06:26:03.746   None     None   \n",
       "\n",
       "                           operation  \\\n",
       "0  CREATE OR REPLACE TABLE AS SELECT   \n",
       "\n",
       "                                 operationParameters   job notebook clusterId  \\\n",
       "0  {'description': None, 'partitionBy': '[]', 'pr...  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend  \\\n",
       "0          NaN           None          False   \n",
       "\n",
       "                                    operationMetrics userMetadata  \n",
       "0  {'numOutputRows': '4', 'numOutputBytes': '2569...         None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE HISTORY {pm.hive}\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a1a3e4",
   "metadata": {},
   "source": [
    "## RESTORE to VERSION 0 ?\n",
    "\n",
    "If you need to start from beginning, you can either run this cell with `restore` being True. This cell will:\n",
    "* Overwrite the VERSION 0 on top of the current version\n",
    "\n",
    "Note that this does not empty the checkpoint_path that is creater below. If you need to reset the checkpoint, delete all files in that directory manually or point it to some other directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df459fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cell skipped due to boolean value.\n"
     ]
    }
   ],
   "source": [
    "# Change me\n",
    "restore = False\n",
    "\n",
    "\n",
    "if restore:\n",
    "    # Get version 0    \n",
    "    df_full_load = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(pm.bronze)\n",
    "\n",
    "    # Load\n",
    "    (\n",
    "        df_full_load\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema', 'true')\n",
    "        .option('path', os.path.abspath(pm.bronze))\n",
    "        .saveAsTable(pm.hive)\n",
    "    )\n",
    "else:\n",
    "    print(\"[INFO] Cell skipped due to boolean value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d0d6a",
   "metadata": {},
   "source": [
    "## Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a762f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Schema is forced to match the Bronze, but with an extra field. DMS does not add Op to full load.\n",
    "readers_schema = spark.read.format(\"delta\").load(pm.bronze).schema.add(\"Op\", \"string\")\n",
    "\n",
    "# Checkpoints will be written to...\n",
    "checkpoint_path = os.path.join('S3', 'bronze', '_checkpoints', 'abc', db, table)\n",
    "\n",
    "\n",
    "# Prepare Spark Auto Loader\n",
    "df = ( spark.readStream\n",
    "        .format(\"parquet\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .option(\"pathGlobFilter\", \"[!L][!O][!A][!D]*.parquet\")\n",
    "        .schema(readers_schema)\n",
    "        .load(pm.staging)\n",
    "  )\n",
    "\n",
    "\n",
    "# Stream\n",
    "streamingquery = ( \n",
    "    df\n",
    "    .writeStream\n",
    "    .trigger(once=True)\n",
    "    .foreachBatch(merge_to_delta)\n",
    "    .option(\"checkpointLocation\", os.path.abspath(checkpoint_path))\n",
    "    .start()\n",
    ")\n",
    " \n",
    "streamingquery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df993501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batchId': 0,\n",
      " 'durationMs': {'addBatch': 5975,\n",
      "                'getBatch': 37,\n",
      "                'latestOffset': 159,\n",
      "                'queryPlanning': 28,\n",
      "                'triggerExecution': 6531,\n",
      "                'walCommit': 148},\n",
      " 'id': '3bf251f8-8ed4-4328-b4f0-d679cf4b28be',\n",
      " 'inputRowsPerSecond': 0.0,\n",
      " 'name': None,\n",
      " 'numInputRows': 12,\n",
      " 'processedRowsPerSecond': 1.837109614206981,\n",
      " 'runId': 'fa2910c0-a334-469c-b717-fe438629588b',\n",
      " 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1},\n",
      " 'sources': [{'description': 'FileStreamSource[file:/C:/Users/soura/PycharmProjects/opinnaytetyo/S3/staging/dms/abc/devices/device_models]',\n",
      "              'endOffset': {'logOffset': 0},\n",
      "              'inputRowsPerSecond': 0.0,\n",
      "              'numInputRows': 12,\n",
      "              'processedRowsPerSecond': 1.837109614206981,\n",
      "              'startOffset': None}],\n",
      " 'stateOperators': [],\n",
      " 'timestamp': '2021-09-12T06:29:08.028Z'}\n"
     ]
    }
   ],
   "source": [
    "# Init\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "# Print what the query performed\n",
    "pp.pprint(streamingquery.lastProgress)\n",
    "\n",
    "# Use for forging the compacted DataFrame later on\n",
    "bid = streamingquery.lastProgress['batchId']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f5781",
   "metadata": {},
   "source": [
    "# Examine before-after compaction\n",
    "\n",
    "**Note:** The HTML class from IPython.core.display is being used to display the Pandas Dataframe without Pandas' index. This index would be an extra row numbering that does not exist in the original dataset, and might cause confusion with the `id` field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d07b65",
   "metadata": {},
   "source": [
    "### Bronze before Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "882ac4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dms_timestamp</th>\n",
       "      <th>id</th>\n",
       "      <th>release_date</th>\n",
       "      <th>name</th>\n",
       "      <th>color</th>\n",
       "      <th>description</th>\n",
       "      <th>created</th>\n",
       "      <th>modified</th>\n",
       "      <th>src_batch_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2021-09-11 11:39:29</td>\n",
       "      <td>2</td>\n",
       "      <td>2010-05-15</td>\n",
       "      <td>Super Gadget 100</td>\n",
       "      <td>Black</td>\n",
       "      <td>update id 2</td>\n",
       "      <td>2010-03-21 12:00:02</td>\n",
       "      <td>2021-09-11 11:39:29</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "      <td>6</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>Super Gadget 300</td>\n",
       "      <td>Pink</td>\n",
       "      <td>new device</td>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-09-11 11:37:47</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-05-15</td>\n",
       "      <td>Super Gadget 100</td>\n",
       "      <td>Red</td>\n",
       "      <td>update id 1</td>\n",
       "      <td>2010-03-21 12:00:01</td>\n",
       "      <td>2021-09-11 11:37:47</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-09-11 11:30:04</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-05-13</td>\n",
       "      <td>Super Gadget 200</td>\n",
       "      <td>White</td>\n",
       "      <td>lorem ipsum</td>\n",
       "      <td>2018-03-20 12:01:01</td>\n",
       "      <td>2018-03-20 12:01:01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-09-11 11:30:04</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>Super Gadget 100</td>\n",
       "      <td>Pink</td>\n",
       "      <td>lorem ipsum</td>\n",
       "      <td>2010-08-05 07:00:00</td>\n",
       "      <td>2010-08-05 07:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the VERSION 0 - The original FULL LOAD.\n",
    "df_orig = spark.table(pm.hive).toPandas()\n",
    "\n",
    "display(HTML(df_orig.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b4718",
   "metadata": {},
   "source": [
    "### CDC Before Log Compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a6dfbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Op</th>\n",
       "      <th>dms_timestamp</th>\n",
       "      <th>id</th>\n",
       "      <th>release_date</th>\n",
       "      <th>name</th>\n",
       "      <th>color</th>\n",
       "      <th>description</th>\n",
       "      <th>created</th>\n",
       "      <th>modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>I</td>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>Super Gadget 300</td>\n",
       "      <td>Black</td>\n",
       "      <td>new device</td>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I</td>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "      <td>6</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>Super Gadget 300</td>\n",
       "      <td>Pink</td>\n",
       "      <td>new device</td>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>U</td>\n",
       "      <td>2021-09-11 11:37:47</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-05-15</td>\n",
       "      <td>Super Gadget 100</td>\n",
       "      <td>Red</td>\n",
       "      <td>update id 1</td>\n",
       "      <td>2010-03-21 12:00:01</td>\n",
       "      <td>2021-09-11 11:37:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>U</td>\n",
       "      <td>2021-09-11 11:37:47</td>\n",
       "      <td>2</td>\n",
       "      <td>2010-05-15</td>\n",
       "      <td>Super Gadget 100</td>\n",
       "      <td>Black</td>\n",
       "      <td>upddddddate id 2</td>\n",
       "      <td>2010-03-21 12:00:02</td>\n",
       "      <td>2021-09-11 11:37:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>U</td>\n",
       "      <td>2021-09-11 11:39:29</td>\n",
       "      <td>2</td>\n",
       "      <td>2010-05-15</td>\n",
       "      <td>Super Gadget 100</td>\n",
       "      <td>Black</td>\n",
       "      <td>update id 2</td>\n",
       "      <td>2010-03-21 12:00:02</td>\n",
       "      <td>2021-09-11 11:39:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>D</td>\n",
       "      <td>2021-09-11 11:43:31</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>Super Gadget 300</td>\n",
       "      <td>Black</td>\n",
       "      <td>new device</td>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "      <td>2021-09-11 11:43:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the original files from staging.\n",
    "df_cdc = (\n",
    "    spark.read.option(\"recursiveFileLookup\", \"true\")\n",
    "    .option(\"pathGlobFilter\", \"[!L][!O][!A][!D]*.parquet\")\n",
    "    .load(pm.staging)\n",
    ").orderBy(\"dms_timestamp\")\n",
    "\n",
    "#Show\n",
    "display(HTML(df_cdc.toPandas().to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84494b37",
   "metadata": {},
   "source": [
    "### CDC After Log Compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eeeb3d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Op</th>\n",
       "      <th>dms_timestamp</th>\n",
       "      <th>id</th>\n",
       "      <th>release_date</th>\n",
       "      <th>name</th>\n",
       "      <th>color</th>\n",
       "      <th>description</th>\n",
       "      <th>created</th>\n",
       "      <th>modified</th>\n",
       "      <th>op_numeral</th>\n",
       "      <th>dms_temp</th>\n",
       "      <th>src_batch_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>U</td>\n",
       "      <td>2021-09-11 11:37:47</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-05-15</td>\n",
       "      <td>Super Gadget 100</td>\n",
       "      <td>Red</td>\n",
       "      <td>update id 1</td>\n",
       "      <td>2010-03-21 12:00:01</td>\n",
       "      <td>2021-09-11 11:37:47</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-09-11 11:37:47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>U</td>\n",
       "      <td>2021-09-11 11:39:29</td>\n",
       "      <td>2</td>\n",
       "      <td>2010-05-15</td>\n",
       "      <td>Super Gadget 100</td>\n",
       "      <td>Black</td>\n",
       "      <td>update id 2</td>\n",
       "      <td>2010-03-21 12:00:02</td>\n",
       "      <td>2021-09-11 11:39:29</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-09-11 11:39:29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>D</td>\n",
       "      <td>2021-09-11 11:43:31</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>Super Gadget 300</td>\n",
       "      <td>Black</td>\n",
       "      <td>new device</td>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "      <td>2021-09-11 11:43:31</td>\n",
       "      <td>3</td>\n",
       "      <td>2021-09-11 11:43:31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I</td>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "      <td>6</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>Super Gadget 300</td>\n",
       "      <td>Pink</td>\n",
       "      <td>new device</td>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add ordering columns and perform compaction\n",
    "df_latest_uniques = log_compact(with_ordering_cols(df_cdc, bid)).orderBy(\"id\")\n",
    "\n",
    "# Show\n",
    "display(HTML(df_latest_uniques.toPandas().to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15042a5",
   "metadata": {},
   "source": [
    "### Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11ee6e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dms_timestamp</th>\n",
       "      <th>id</th>\n",
       "      <th>release_date</th>\n",
       "      <th>name</th>\n",
       "      <th>color</th>\n",
       "      <th>description</th>\n",
       "      <th>created</th>\n",
       "      <th>modified</th>\n",
       "      <th>src_batch_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2021-09-11 11:37:47</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-05-15</td>\n",
       "      <td>Super Gadget 100</td>\n",
       "      <td>Red</td>\n",
       "      <td>update id 1</td>\n",
       "      <td>2010-03-21 12:00:01</td>\n",
       "      <td>2021-09-11 11:37:47</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-09-11 11:39:29</td>\n",
       "      <td>2</td>\n",
       "      <td>2010-05-15</td>\n",
       "      <td>Super Gadget 100</td>\n",
       "      <td>Black</td>\n",
       "      <td>update id 2</td>\n",
       "      <td>2010-03-21 12:00:02</td>\n",
       "      <td>2021-09-11 11:39:29</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-09-11 11:30:04</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>Super Gadget 100</td>\n",
       "      <td>Pink</td>\n",
       "      <td>lorem ipsum</td>\n",
       "      <td>2010-08-05 07:00:00</td>\n",
       "      <td>2010-08-05 07:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-09-11 11:30:04</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-05-13</td>\n",
       "      <td>Super Gadget 200</td>\n",
       "      <td>White</td>\n",
       "      <td>lorem ipsum</td>\n",
       "      <td>2018-03-20 12:01:01</td>\n",
       "      <td>2018-03-20 12:01:01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "      <td>6</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>Super Gadget 300</td>\n",
       "      <td>Pink</td>\n",
       "      <td>new device</td>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "      <td>2021-09-11 11:37:17</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(spark.sql(f\"SELECT * FROM {pm.hive} ORDER BY id\").toPandas().to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19e1f2",
   "metadata": {},
   "source": [
    "## Delta History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "698aff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DeltaTable.forName(spark, pm.hive)\n",
    "\n",
    "dt_hist = dt.history().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6d36070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>operationMetrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2021-09-12 06:26:03.746</td>\n",
       "      <td>CREATE OR REPLACE TABLE AS SELECT</td>\n",
       "      <td>{'description': None, 'partitionBy': '[]', 'properties': '{}', 'isManaged': 'false'}</td>\n",
       "      <td>{'numOutputRows': '4', 'numOutputBytes': '2569', 'numFiles': '1'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2021-09-12 06:29:13.334</td>\n",
       "      <td>MERGE</td>\n",
       "      <td>{'matchedPredicates': '[{\"predicate\":\"(s.`Op` = 'D')\",\"actionType\":\"delete\"},{\"predicate\":\"(s.`Op` = 'U')\",\"actionType\":\"update\"}]', 'predicate': '(t.`id` = s.`id`)', 'notMatchedPredicates': '[{\"predicate\":\"(NOT (s.`Op` = 'D'))\",\"actionType\":\"insert\"}]'}</td>\n",
       "      <td>{'numOutputRows': '5', 'numTargetRowsInserted': '1', 'numTargetRowsUpdated': '2', 'numTargetFilesAdded': '6', 'numTargetFilesRemoved': '1', 'numTargetRowsDeleted': '0', 'scanTimeMs': '2358', 'numSourceRows': '4', 'executionTimeMs': '4477', 'numTargetRowsCopied': '2', 'rewriteTimeMs': '2114'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols_to_display = ['version', 'timestamp', 'operation', 'operationParameters', 'operationMetrics']\n",
    "\n",
    "dt_hist[cols_to_display]\n",
    "\n",
    "\n",
    "# Show\n",
    "display(HTML(dt_hist[cols_to_display].sort_values('version').to_html(index=False)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
