{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e9ec06",
   "metadata": {},
   "source": [
    "# Merge the CDC Files to Bronze\n",
    "\n",
    "Note that in Databricks, we could use the Auto Loader. In this case, our **readStream** would have additional options:\n",
    "\n",
    "```python\n",
    " .format(\"cloudFiles\")\n",
    " .option(\"cloudFiles.format\", \"parquet\")\n",
    " .option(\"cloudFiles.useNotifications\", \"true\") # Use for SQS/SNS\n",
    " .option(\"cloudFiles.region\", \"eu-west-1\")      # Use for SQS/SNS\n",
    "```\n",
    "\n",
    "My thesis includes examples of this in Finnish. This script uses file listing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c8621f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "from helpers.paths import PathMerger\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52978122",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"MergeCDCtoBronze\")\n",
    "         .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.8.0\")\n",
    "         .config('spark.sql.extensions', \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "         .config('spark.sql.session.timeZone', 'UTC')\n",
    "         .getOrCreate())\n",
    "\n",
    "\n",
    "# This cannot be imported before initializing the SparkSession.\n",
    "from delta import DeltaTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c68258",
   "metadata": {},
   "source": [
    "## Imagine orchestator here\n",
    "\n",
    "If this was a worker Notebook in Databricks, or a worker Python script orchestrated by Airflow the parameters below would be fed while executing this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dbbcfcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "db, table = 'devices', 'device_models'\n",
    "all_pks = [\"id\"]\n",
    "\n",
    "\n",
    "# Init\n",
    "pm = PathMerger(db, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b6f18939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The following Parquet files exist in this staging path: \n",
      "S3\\staging\\dms\\abc\\devices\\device_models\\LOAD00000001.parquet\n",
      "S3\\staging\\dms\\abc\\devices\\device_models\\2021\\8\\19\\20210819_104713.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] The following Parquet files exist in this staging path: \")\n",
    "\n",
    "for f in glob.glob(pm.staging + os.sep + \"**/*.parquet\", recursive=True):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b80e0d",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "Based on my testing, the pathGlobFilter applies to the filename, not to the whole path. \n",
    "\n",
    "Thus, a glob filter such as...\n",
    "* `**/*.parquet` returns no files\n",
    "* `[L]*.parquet` returns all files starting with an `L` letter and ending to `.parquet`.\n",
    "* `[!L]*.parquet` returns all files NOT starting with an `L` letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dda0cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = None\n",
    "merge_df = None\n",
    "\n",
    "def merge_to_delta(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    TODO: Write what this does.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Not to use in production. Steal the DataFrame so that we can investigate it outside this function.\n",
    "    global input_df\n",
    "    input_df = batch_df\n",
    "    \n",
    "    # Add op_numeral\n",
    "    df_batch = (\n",
    "        m_df\n",
    "         .withColumn(\"op_numeral\", F.when(F.col(\"Op\") == \"I\", 1)\n",
    "                                     .when(F.col(\"Op\") == \"U\", 2)\n",
    "                                     .when(F.col(\"Op\") == \"D\", 3).cast(\"int\")) # Temporary column\n",
    "        .withColumn('dms_temp', F.to_timestamp(F.col(\"dms_timestamp\"))) # Temporary column\n",
    "        # .withColumn(\"par\", F.col(\"*all_pks[0]) % n_pars)\n",
    "        # .withColumn(\"src_file\", F.input_file_name())\n",
    "        .withColumn(\"src_batch_id\", F.lit(batch_id))\n",
    "    )\n",
    "\n",
    "    # These two, as well as Op, are not available in the target Delta Table. Mark as to-be-dropped.\n",
    "    cols_to_drop = [\"dms_temp\", \"op_numeral\"]\n",
    "    \n",
    "    latest_uniques = (\n",
    "        df_batch\n",
    "            .selectExpr(*all_pks, \"struct(dms_temp, op_numeral, *) as others\")\n",
    "            .groupBy(*all_pks)\n",
    "            .agg(F.max(\"others\").alias(\"latest\"))\n",
    "            .select(\"latest.*\")\n",
    "            .drop(*cols_to_drop)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a762f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Schema is forced to match the Bronze, but with an extra field. DMS does not add Op to full load.\n",
    "readers_schema = spark.read.format(\"delta\").load(pm.bronze).schema.add(\"Op\", \"string\")\n",
    "\n",
    "# Checkpoints will be written to...\n",
    "checkpoint_path = os.path.join('S3', 'bronze', '_checkpoints', 'abc', db, table)\n",
    "    \n",
    "# Prepare Spark Auto Loader\n",
    "df = ( spark.readStream\n",
    "        .format(\"parquet\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .option(\"pathGlobFilter\", \"[!L][!O][!A][!D]*.parquet\")\n",
    "        .schema(readers_schema)\n",
    "        .load(pm.staging)\n",
    "  )\n",
    "\n",
    "\n",
    "# Merge to Bronze\n",
    "\n",
    "\n",
    "streamingquery = ( \n",
    "    df\n",
    "    .writeStream\n",
    "    .trigger(once=True)\n",
    "    .foreachBatch(merge_to_delta)\n",
    "    # .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .start()\n",
    ")\n",
    " \n",
    "streamingquery.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f5781",
   "metadata": {},
   "source": [
    "## Examine before-after compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7f2f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ffa7b9c",
   "metadata": {},
   "source": [
    "## TODO: Examine query progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "36d7e251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '78c08a94-cc04-4cf0-86d7-0991ce907e96',\n",
       " 'runId': 'e23cad9b-d0a4-4f6e-8d69-551bc16e224c',\n",
       " 'name': None,\n",
       " 'timestamp': '2021-08-19T07:35:59.064Z',\n",
       " 'batchId': 0,\n",
       " 'numInputRows': 8,\n",
       " 'inputRowsPerSecond': 0.0,\n",
       " 'processedRowsPerSecond': 4.640371229698376,\n",
       " 'durationMs': {'addBatch': 252,\n",
       "  'getBatch': 16,\n",
       "  'latestOffset': 538,\n",
       "  'queryPlanning': 0,\n",
       "  'triggerExecution': 1724,\n",
       "  'walCommit': 424},\n",
       " 'stateOperators': [],\n",
       " 'sources': [{'description': 'FileStreamSource[file:/C:/Users/soura/PycharmProjects/opinnaytetyo/S3/staging/dms/abc/devices/device_models]',\n",
       "   'startOffset': None,\n",
       "   'endOffset': {'logOffset': 0},\n",
       "   'numInputRows': 8,\n",
       "   'inputRowsPerSecond': 0.0,\n",
       "   'processedRowsPerSecond': 4.640371229698376}],\n",
       " 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingquery.lastProgress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
